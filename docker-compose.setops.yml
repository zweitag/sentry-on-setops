version: "3.4"
x-sentry-defaults: &sentry_defaults
  build:
    context: ./
    dockerfile: Dockerfile.sentry
    args:
        SENTRY_IMAGE: "$SENTRY_IMAGE"
  volumes:
    - "sentry-data:/data"
    - "sentry-geoip:/geoip:ro"
  depends_on:
    - redis
    - postgres
    - memcached
    - snuba-api
    - snuba-consumer
    - snuba-outcomes-consumer
    - snuba-sessions-consumer
    - snuba-transactions-consumer
    - snuba-subscription-consumer-events
    - snuba-subscription-consumer-transactions
    - snuba-replacer
    - symbolicator
    - kafka
x-snuba-defaults: &snuba_defaults
  depends_on:
    - redis
    - clickhouse
    - kafka
  image: "$SNUBA_IMAGE"
  environment:
    SNUBA_SETTINGS: docker
    CLICKHOUSE_HOST: clickhouse
    DEFAULT_BROKERS: "kafka:9092"
    REDIS_HOST: redis
    UWSGI_MAX_REQUESTS: "10000"
    UWSGI_DISABLE_LOGGING: "true"
    # Leaving the value empty to just pass whatever is set
    # on the host system (or in the .env file)
    SENTRY_EVENT_RETENTION_DAYS:
services:
  # smtp:
  #   image: tianon/exim4
  #   volumes:
  #     - "sentry-smtp:/var/spool/exim4"
  #     - "sentry-smtp-log:/var/log/exim4"
  memcached:
    image: "memcached:1.5-alpine"
  redis:
    image: "redis:5.0-alpine"
    volumes:
      - "sentry-redis:/data"
  postgres:
    image: "postgres:9.6"
    environment:
      POSTGRES_HOST_AUTH_METHOD: "trust"
    volumes:
      - "sentry-postgres:/var/lib/postgresql/data"
  zookeeper:
    build:
      context: ./zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: "2181"
      CONFLUENT_SUPPORT_METRICS_ENABLE: "false"
      ZOOKEEPER_LOG4J_ROOT_LOGLEVEL: "WARN"
      ZOOKEEPER_TOOLS_LOG4J_LOGLEVEL: "WARN"
    volumes:
      - "sentry-zookeeper:/var/lib/zookeeper/data"
      - "sentry-zookeeper-log:/var/lib/zookeeper/log"
      - "sentry-secrets:/etc/zookeeper/secrets"
  kafka:
    build:
      context: ./kafka
    environment:
      KAFKA_ZOOKEEPER_CONNECT: "zookeeper:2181"
      KAFKA_ADVERTISED_LISTENERS: "PLAINTEXT://kafka:9092"
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: "1"
      KAFKA_OFFSETS_TOPIC_NUM_PARTITIONS: "1"
      KAFKA_LOG_RETENTION_HOURS: "24"
      KAFKA_MESSAGE_MAX_BYTES: "50000000" #50MB or bust
      KAFKA_MAX_REQUEST_SIZE: "50000000" #50MB on requests apparently too
      CONFLUENT_SUPPORT_METRICS_ENABLE: "false"
      KAFKA_LOG4J_LOGGERS: "kafka.cluster=WARN,kafka.controller=WARN,kafka.coordinator=WARN,kafka.log=WARN,kafka.server=WARN,kafka.zookeeper=WARN,state.change.logger=WARN"
      KAFKA_LOG4J_ROOT_LOGLEVEL: "WARN"
      KAFKA_TOOLS_LOG4J_LOGLEVEL: "WARN"
    volumes:
      - "sentry-kafka:/var/lib/kafka/data"
      - "sentry-kafka-log:/var/lib/kafka/log"
      - "sentry-secrets:/etc/kafka/secrets"
    depends_on:
      - zookeeper
  clickhouse:
    build:
      context: ./clickhouse
    volumes:
      - "sentry-clickhouse:/var/lib/clickhouse"
      - "sentry-clickhouse-log:/var/log/clickhouse-server"
    environment:
      # This limits Clickhouse's memory to 30% of the host memory
      # If you have high volume and your search return incomplete results
      # You might want to change this to a higher value (and ensure your host has enough memory)
      MAX_MEMORY_USAGE_RATIO: 0.3
  geoipupdate:
    build:
      context: ./geoip
    volumes:
      - "sentry-geoip:/sentry"
  snuba-api:
    <<: *snuba_defaults
  # Kafka consumer responsible for feeding events into Clickhouse
  snuba-consumer:
    <<: *snuba_defaults
    command: consumer --storage events --auto-offset-reset=latest --max-batch-time-ms 750
  # Kafka consumer responsible for feeding outcomes into Clickhouse
  # Use --auto-offset-reset=earliest to recover up to 7 days of TSDB data
  # since we did not do a proper migration
  snuba-outcomes-consumer:
    <<: *snuba_defaults
    command: consumer --storage outcomes_raw --auto-offset-reset=earliest --max-batch-time-ms 750
  # Kafka consumer responsible for feeding session data into Clickhouse
  snuba-sessions-consumer:
    <<: *snuba_defaults
    command: consumer --storage sessions_raw --auto-offset-reset=latest --max-batch-time-ms 750
  # Kafka consumer responsible for feeding transactions data into Clickhouse
  snuba-transactions-consumer:
    <<: *snuba_defaults
    command: consumer --storage transactions --consumer-group transactions_group --auto-offset-reset=latest --max-batch-time-ms 750 --commit-log-topic=snuba-commit-log
  snuba-replacer:
    <<: *snuba_defaults
    command: replacer --storage events --auto-offset-reset=latest --max-batch-size 3
  snuba-subscription-consumer-events:
    <<: *snuba_defaults
    command: subscriptions --auto-offset-reset=latest --consumer-group=snuba-events-subscriptions-consumers --topic=events --result-topic=events-subscription-results --dataset=events --commit-log-topic=snuba-commit-log --commit-log-group=snuba-consumers --delay-seconds=60 --schedule-ttl=60
  snuba-subscription-consumer-transactions:
    <<: *snuba_defaults
    command: subscriptions --auto-offset-reset=latest --consumer-group=snuba-transactions-subscriptions-consumers --topic=events --result-topic=transactions-subscription-results --dataset=transactions --commit-log-topic=snuba-commit-log --commit-log-group=transactions_group --delay-seconds=60 --schedule-ttl=60
  snuba-cleanup:
    <<: *snuba_defaults
    build:
      context: ./cron
      args:
        BASE_IMAGE: "$SNUBA_IMAGE"
    command: '"*/5 * * * * gosu snuba snuba cleanup --dry-run False"'
  symbolicator:
    build:
      context: ./symbolicator
      args:
        SYMBOLICATOR_IMAGE: "$SYMBOLICATOR_IMAGE"
    volumes:
      - "sentry-symbolicator:/data"
  symbolicator-cleanup:
    build:
      context: ./cron
      args:
        BASE_IMAGE: "$SYMBOLICATOR_IMAGE"
    command: '"55 23 * * * gosu symbolicator symbolicator cleanup"'
    volumes:
      - "sentry-symbolicator:/data"
  # web:
  #   <<: *sentry_defaults
  # cron:
  #   <<: *sentry_defaults
  #   command: run cron
  # worker:
  #   <<: *sentry_defaults
  #   command: run worker
  # ingest-consumer:
  #   <<: *sentry_defaults
  #   command: run ingest-consumer --all-consumer-types
  # post-process-forwarder:
  #   <<: *sentry_defaults
  #   # Increase `--commit-batch-size 1` below to deal with high-load environments.
  #   command: run post-process-forwarder --commit-batch-size 1
  # subscription-consumer-events:
  #   <<: *sentry_defaults
  #   command: run query-subscription-consumer --commit-batch-size 1 --topic events-subscription-results
  # subscription-consumer-transactions:
  #   <<: *sentry_defaults
  #   command: run query-subscription-consumer --commit-batch-size 1 --topic transactions-subscription-results
  # sentry-cleanup:
  #   <<: *sentry_defaults
  #   build:
  #     context: ./cron
  #     args:
  #       BASE_IMAGE: "$SENTRY_IMAGE"
  #   entrypoint: "/entrypoint.sh"
  #   command: '"0 0 * * * gosu sentry sentry cleanup --days $SENTRY_EVENT_RETENTION_DAYS"'
  nginx:
    ports:
      - "$SENTRY_BIND:80/tcp"
    build:
      context: ./nginx
    depends_on:
      # - web
      - relay
  relay:
    build:
      context: ./
      dockerfile: Dockerfile.relay
      args:
        RELAY_IMAGE: "$RELAY_IMAGE"
    volumes:
      - "sentry-geoip:/geoip:ro"
    depends_on:
      - kafka
      - redis

volumes:
  sentry-data:
  sentry-postgres:
  sentry-redis:
  sentry-zookeeper:
  sentry-kafka:
  sentry-clickhouse:
  sentry-symbolicator:
  sentry-secrets:
  sentry-geoip:
  sentry-zookeeper-log:
  sentry-kafka-log:
  sentry-clickhouse-log:
